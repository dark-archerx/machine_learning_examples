{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/dark-archerx/machine_learning_examples/blob/master/RNN.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "-vlaRloaFzS7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Network\n",
        "\n",
        "This is a pure numpy implementation of word generation using an RNN\n",
        "\n",
        "![alt text](http://corochann.com/wp-content/uploads/2017/05/text_sequence_predict.png \"Logo Title Text 1\")\n",
        "\n",
        "We're going to have our network learn how to predict the next words in a given paragraph. This will require a recurrent architecture since the network will have to remember a sequence of characters. The order matters. 1000 iterations and we'll have pronouncable english. The longer the training time the better. You can feed it any text sequence (words, python, HTML, etc.)\n",
        "\n",
        "## What is a Recurrent Network?\n",
        "\n",
        "Feedforward networks are great for learning a pattern between a set of inputs and outputs.\n",
        "![alt text](https://www.researchgate.net/profile/Sajad_Jafari3/publication/275334508/figure/fig1/AS:294618722783233@1447253985297/Fig-1-Schematic-of-the-multilayer-feed-forward-neural-network-proposed-to-model-the.png \"Logo Title Text 1\")\n",
        "\n",
        "![alt text](https://s-media-cache-ak0.pinimg.com/236x/10/29/a9/1029a9a0534a768b4c4c2b5341bdd003--city-year-math-patterns.jpg \"Logo Title Text 1\")\n",
        "\n",
        "![alt text](https://www.researchgate.net/profile/Hamza_Guellue/publication/223079746/figure/fig5/AS:305255788105731@1449790059371/Fig-5-Configuration-of-a-three-layered-feed-forward-neural-network.png\n",
        " \"Logo Title Text 1\")\n",
        "\n",
        "- temperature & location\n",
        "- height & weight\n",
        "- car speed and brand\n",
        "\n",
        "But what if the ordering of the data matters? \n",
        "\n",
        "![alt text](http://www.aboutcurrency.com/images/university/fxvideocourse/google_chart.jpg \"Logo Title Text 1\")\n",
        "\n",
        "![alt text](http://news.mit.edu/sites/mit.edu.newsoffice/files/styles/news_article_image_top_slideshow/public/images/2016/vondrick-machine-learning-behavior-algorithm-mit-csail_0.jpg?itok=ruGmLJm2 \"Logo Title Text 1\")\n",
        "\n",
        "Alphabet, Lyrics of a song. These are stored using Conditional Memory. You can only access an element if you have access to the previous elements (like a linkedlist). \n",
        "\n",
        "Enter recurrent networks\n",
        "\n",
        "We feed the hidden state from the previous time step back into the the network at the next time step.\n",
        "\n",
        "![alt text](https://iamtrask.github.io/img/basic_recurrence_singleton.png \"Logo Title Text 1\")\n",
        "\n",
        "So instead of the data flow operation happening like this\n",
        "\n",
        "## input -> hidden -> output\n",
        "\n",
        "it happens like this\n",
        "\n",
        "## (input + prev_hidden) -> hidden -> output\n",
        "\n",
        "wait. Why not this?\n",
        "\n",
        "## (input + prev_input) -> hidden -> output\n",
        "\n",
        "Hidden recurrence learns what to remember whereas input recurrence is hard wired to just remember the immediately previous datapoint\n",
        "\n",
        "![alt text](https://image.slidesharecdn.com/ferret-rnn-151211092908/95/recurrent-neural-networks-part-1-theory-10-638.jpg?cb=1449826311 \"Logo Title Text 1\")\n",
        "\n",
        "![alt text](https://www.mathworks.com/help/examples/nnet/win64/RefLayRecNetExample_01.png \"Logo Title Text 1\")\n",
        "\n",
        "RNN Formula\n",
        "![alt text](https://cdn-images-1.medium.com/max/1440/0*TUFnE2arCrMrCvxH.png \"Logo Title Text 1\")\n",
        "\n",
        "It basically says the current hidden state h(t) is a function f of the previous hidden state h(t-1) and the current input x(t). The theta are the parameters of the function f. The network typically learns to use h(t) as a kind of lossy summary of the task-relevant aspects of the past sequence of inputs up to t.\n",
        "\n",
        "Loss function\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1440/0*ZsEG2aWfgqtk9Qk5. \"Logo Title Text 1\")\n",
        "\n",
        "The total loss for a given sequence of x values paired with a sequence of y values would then be just the sum of the losses over all the time steps. For example, if L(t) is the negative log-likelihood\n",
        "of y (t) given x (1), . . . , x (t) , then sum them up you get the loss for the sequence \n",
        "\n",
        "\n",
        "## Our steps\n",
        "\n",
        "- Initialize weights randomly\n",
        "- Give the model a char pair (input char & target char. The target char is the char the network should guess, its the next char in our sequence)\n",
        "- Forward pass (We calculate the probability for every possible next char according to the state of the model, using the paramters)\n",
        "- Measure error (the distance between the previous probability and the target char)\n",
        "- We calculate gradients for each of our parameters to see their impact they have on the loss (backpropagation through time)\n",
        "- update all parameters in the direction via gradients that help to minimise the loss\n",
        "- Repeat! Until our loss is small AF\n",
        "\n",
        "## What are some use cases?\n",
        "\n",
        "- Time series prediction (weather forecasting, stock prices, traffic volume, etc. )\n",
        "- Sequential data generation (music, video, audio, etc.)\n",
        "\n",
        "## Other Examples\n",
        "\n",
        "-https://github.com/anujdutt9/RecurrentNeuralNetwork (binary addition)\n",
        "\n",
        "## What's next? \n",
        "\n",
        "1 LSTM Networks\n",
        "2 Bidirectional networks\n",
        "3 recursive networks"
      ]
    },
    {
      "metadata": {
        "id": "CE8FMyeCMBXz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#katka.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zTd8n4i6F-h9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e26abe45-27fc-47a4-eccd-01b62857bbe9"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!wget https://raw.githubusercontent.com/llSourcell/recurrent_neural_network/master/kafka.txt"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-05-09 04:16:14--  https://raw.githubusercontent.com/llSourcell/recurrent_neural_network/master/kafka.txt\r\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\r\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 137629 (134K) [text/plain]\n",
            "Saving to: ‘kafka.txt.1’\n",
            "\n",
            "kafka.txt.1         100%[===================>] 134.40K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2018-05-09 04:16:14 (5.84 MB/s) - ‘kafka.txt.1’ saved [137629/137629]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zevwI4H9MgGi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "6048RKPLMgWd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "c5d2d602-6185-4612-f4f6-a85aea2a5334"
      },
      "cell_type": "code",
      "source": [
        "!wget https://gist.githubusercontent.com/dark-archerx/ab35dc4e947164dd8303ec14939e8448/raw/c4e375eb67feaee7482eebd1c717ec4d2fa15b31/resume.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-05-10 13:16:34--  https://gist.githubusercontent.com/dark-archerx/ab35dc4e947164dd8303ec14939e8448/raw/c4e375eb67feaee7482eebd1c717ec4d2fa15b31/resume.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 110931 (108K) [text/plain]\n",
            "Saving to: ‘resume.txt’\n",
            "\n",
            "resume.txt          100%[===================>] 108.33K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2018-05-10 13:16:34 (5.15 MB/s) - ‘resume.txt’ saved [110931/110931]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pnf3tq-LFzS_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The code contains 4 parts\n",
        "* Load the trainning data\n",
        "  * encode char into vectors\n",
        "* Define the Recurrent Network\n",
        "* Define a loss function\n",
        "  * Forward pass\n",
        "  * Loss\n",
        "  * Backward pass\n",
        "* Define a function to create sentences from the model\n",
        "* Train the network\n",
        "  * Feed the network\n",
        "  * Calculate gradient and update the model parameters\n",
        "  * Output a text to see the progress of the training\n",
        " \n",
        "\n",
        "## Load the training data\n",
        "\n",
        "The network need a big txt file as an input.\n",
        "\n",
        "The content of the file will be used to train the network.\n",
        "\n",
        "I use Methamorphosis from Kafka (Public Domain). Because Kafka was one weird dude. I like."
      ]
    },
    {
      "metadata": {
        "id": "6cQ8cPqCFzTB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4af751a6-7c03-42a8-aaf2-57e4088b330f"
      },
      "cell_type": "code",
      "source": [
        "data = open('resume.txt', 'r').read()\n",
        "\n",
        "chars = list(set(data)) \n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print 'data has %d chars, %d unique' % (data_size, vocab_size)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data has 110931 chars, 111 unique\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7R6yEZhrAGbO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8cys3SOXAUmf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dm6oilJzFzTL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encode/Decode char/vector\n",
        "\n",
        "Neural networks operate on vectors (a vector is an array of float)\n",
        "So we need a way to encode and decode a char as a vector.\n",
        "\n",
        "We'll count the number of unique chars (*vocab_size*). That will be the size of the vector. \n",
        "The vector contains only zero exept for the position of the char wherae the value is 1.\n",
        "\n",
        "#### So First let's calculate the *vocab_size*:"
      ]
    },
    {
      "metadata": {
        "id": "Mou7AsYPFzTM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "82ec2a0a-0e4e-481d-af12-192a872e7092"
      },
      "cell_type": "code",
      "source": [
        "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
        "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
        "print char_to_ix\n",
        "print ix_to_char"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\\x8b': 0, '\\x93': 1, ' ': 2, '$': 3, '(': 4, ',': 5, '0': 6, '\\xb3': 7, '4': 8, '8': 9, '@': 10, '\\xc3': 11, 'D': 12, 'H': 13, 'L': 14, 'P': 15, 'T': 16, 'X': 17, '\\\\': 18, '`': 19, 'd': 20, 'h': 21, 'l': 22, 'p': 23, 't': 24, 'x': 25, '|': 26, '\\x80': 27, '\\x98': 28, '\\x9c': 29, '#': 30, \"'\": 31, '+': 32, '/': 33, '3': 34, '7': 35, ';': 36, '?': 37, 'C': 38, 'G': 39, 'K': 40, 'O': 41, 'S': 42, 'W': 43, '[': 44, '_': 45, 'c': 46, 'g': 47, 'k': 48, 'o': 49, 's': 50, 'w': 51, '{': 52, '\\x81': 53, '\\n': 54, '\\x99': 55, '\\x9d': 56, '\"': 57, '&': 58, '*': 59, '.': 60, '2': 61, '6': 62, ':': 63, '>': 64, 'B': 65, '\\xc5': 66, 'F': 67, 'J': 68, 'N': 69, 'R': 70, 'V': 71, 'Z': 72, '^': 73, 'b': 74, 'f': 75, 'j': 76, 'n': 77, 'r': 78, 'v': 79, 'z': 80, '~': 81, '\\t': 82, '\\x96': 83, '!': 84, '\\xa2': 85, '%': 86, ')': 87, '-': 88, '1': 89, '5': 90, '9': 91, '\\xba': 92, '=': 93, 'A': 94, 'E': 95, 'I': 96, 'M': 97, 'Q': 98, 'U': 99, 'Y': 100, ']': 101, 'a': 102, '\\xe2': 103, 'e': 104, 'i': 105, 'm': 106, 'q': 107, 'u': 108, 'y': 109, '}': 110}\n",
            "{0: '\\x8b', 1: '\\x93', 2: ' ', 3: '$', 4: '(', 5: ',', 6: '0', 7: '\\xb3', 8: '4', 9: '8', 10: '@', 11: '\\xc3', 12: 'D', 13: 'H', 14: 'L', 15: 'P', 16: 'T', 17: 'X', 18: '\\\\', 19: '`', 20: 'd', 21: 'h', 22: 'l', 23: 'p', 24: 't', 25: 'x', 26: '|', 27: '\\x80', 28: '\\x98', 29: '\\x9c', 30: '#', 31: \"'\", 32: '+', 33: '/', 34: '3', 35: '7', 36: ';', 37: '?', 38: 'C', 39: 'G', 40: 'K', 41: 'O', 42: 'S', 43: 'W', 44: '[', 45: '_', 46: 'c', 47: 'g', 48: 'k', 49: 'o', 50: 's', 51: 'w', 52: '{', 53: '\\x81', 54: '\\n', 55: '\\x99', 56: '\\x9d', 57: '\"', 58: '&', 59: '*', 60: '.', 61: '2', 62: '6', 63: ':', 64: '>', 65: 'B', 66: '\\xc5', 67: 'F', 68: 'J', 69: 'N', 70: 'R', 71: 'V', 72: 'Z', 73: '^', 74: 'b', 75: 'f', 76: 'j', 77: 'n', 78: 'r', 79: 'v', 80: 'z', 81: '~', 82: '\\t', 83: '\\x96', 84: '!', 85: '\\xa2', 86: '%', 87: ')', 88: '-', 89: '1', 90: '5', 91: '9', 92: '\\xba', 93: '=', 94: 'A', 95: 'E', 96: 'I', 97: 'M', 98: 'Q', 99: 'U', 100: 'Y', 101: ']', 102: 'a', 103: '\\xe2', 104: 'e', 105: 'i', 106: 'm', 107: 'q', 108: 'u', 109: 'y', 110: '}'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0n2ZToC-FzTS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Then we create 2 dictionary to encode and decode a char to an int"
      ]
    },
    {
      "metadata": {
        "id": "4ArhabvVFzTU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_6mC6BEpFzTZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Finaly we create a vector from a char like this:\n",
        "The dictionary defined above allosw us to create a vector of size 61 instead of 256.  \n",
        "Here and exemple of the char 'a'  \n",
        "The vector contains only zeros, except at position char_to_ix['a'] where we put a 1."
      ]
    },
    {
      "metadata": {
        "id": "cbn27bYPFzTZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "aba6609f-1268-4753-93b2-c65430eab70a"
      },
      "cell_type": "code",
      "source": [
        "# like one hot encoding\n",
        "import numpy as np\n",
        "\n",
        "vector_for_char_a = np.zeros((vocab_size, 1))\n",
        "vector_for_char_a[char_to_ix['a']] = 1\n",
        "print vector_for_char_a.ravel()\n",
        "ch=np.array(chars)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ughgwgGBdB6c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OtOxT50tFzTf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Definition of the network\n",
        "\n",
        "The neural network is made of 3 layers:\n",
        "* an input layer\n",
        "* an hidden layer\n",
        "* an output layer\n",
        "\n",
        "All layers are fully connected to the next one: each node of a layer are conected to all nodes of the next layer.\n",
        "The hidden layer is connected to the output and to itself: the values from an iteration are used for the next one.\n",
        "\n",
        "To centralise values that matter for the training (_hyper parameters_) we also define the _sequence lenght_ and the _learning rate_"
      ]
    },
    {
      "metadata": {
        "id": "gWTv6zdYFzTf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sKw-y-5zFzTl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model parameters\n",
        "\n",
        "hidden_size = 200\n",
        "seq_length = 50\n",
        "learning_rate = 1e-1\n",
        "\n",
        "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
        "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
        "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #input to hidden\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((vocab_size, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4qLO5C2tFzTq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model parameters are adjusted during the trainning.\n",
        "* _Wxh_ are parameters to connect a vector that contain one input to the hidden layer.\n",
        "* _Whh_ are parameters to connect the hidden layer to itself. This is the Key of the Rnn: Recursion is done by injecting the previous values from the output of the hidden state, to itself at the next iteration.\n",
        "* _Why_ are parameters to connect the hidden layer to the output\n",
        "* _bh_ contains the hidden bias\n",
        "* _by_ contains the output bias\n",
        "\n",
        "You'll see in the next section how theses parameters are used to create a sentence."
      ]
    },
    {
      "metadata": {
        "id": "z9jR11J6FzTs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the loss function\n",
        "\n",
        "The __loss__ is a key concept in all neural networks training. \n",
        "It is a value that describe how good is our model.  \n",
        "The smaller the loss, the better our model is.  \n",
        "(A good model is a model where the predicted output is close to the training output)\n",
        "  \n",
        "During the training phase we want to minimize the loss.\n",
        "\n",
        "The loss function calculates the loss but also the gradients (see backward pass):\n",
        "* It perform a forward pass: calculate the next char given a char from the training set.\n",
        "* It calculate the loss by comparing the predicted char to the target char. (The target char is the input following char in the tranning set)\n",
        "* It calculate the backward pass to calculate the gradients \n",
        "\n",
        "This function take as input:\n",
        "* a list of input char\n",
        "* a list of target char\n",
        "* and the previous hidden state\n",
        "\n",
        "This function outputs:\n",
        "* the loss\n",
        "* the gradient for each parameters between layers\n",
        "* the last hidden state\n"
      ]
    },
    {
      "metadata": {
        "id": "sazqs8tYFzTt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Forward pass\n",
        "The forward pass use the parameters of the model (Wxh, Whh, Why, bh, by) to calculate the next char given a char from the trainning set.\n",
        "\n",
        "xs[t] is the vector that encode the char at position t\n",
        "ps[t] is the probabilities for next char\n",
        "\n",
        "![alt text](https://deeplearning4j.org/img/recurrent_equation.png \"Logo Title Text 1\")\n",
        "\n",
        "```python\n",
        "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
        "ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
        "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
        "```\n",
        "\n",
        "or is dirty pseudo code for each char\n",
        "```python\n",
        "hs = input*Wxh + last_value_of_hidden_state*Whh + bh\n",
        "ys = hs*Why + by\n",
        "ps = normalized(ys)\n",
        "```\n",
        "\n",
        "### Backward pass\n",
        "\n",
        "The naive way to calculate all gradients would be to recalculate a loss for small variations for each parameters.\n",
        "This is possible but would be time consuming.\n",
        "There is a technics to calculates all the gradients for all the parameters at once: the backdrop propagation.  \n",
        "Gradients are calculated in the oposite order of the forward pass, using simple technics.  \n",
        "\n",
        "#### goal is to calculate gradients for the forward formula:\n",
        "```python\n",
        "hs = input*Wxh + last_value_of_hidden_state*Whh + bh  \n",
        "ys = hs*Why + by\n",
        "```\n",
        "\n",
        "The loss for one datapoint\n",
        "![alt text](http://i.imgur.com/LlIMvek.png \"Logo Title Text 1\")\n",
        "\n",
        "How should the computed scores inside f change tto decrease the loss? We'll need to derive a gradient to figure that out.\n",
        "\n",
        "Since all output units contribute to the error of each hidden unit we sum up all the gradients calculated at each time step in the sequence and use it to update the parameters. So our parameter gradients becomes :\n",
        "\n",
        "![alt text](http://i.imgur.com/Ig9WGqP.png \"Logo Title Text 1\")\n",
        "\n",
        "Our first gradient of our loss. We'll backpropagate this via chain rule\n",
        "\n",
        "![alt text](http://i.imgur.com/SOJcNLg.png \"Logo Title Text 1\")\n",
        "\n",
        "The chain rule is a method for finding the derivative of composite functions, or functions that are made by combining one or more functions.\n",
        "\n",
        "![alt text](http://i.imgur.com/3Z2Rfdi.png \"Logo Title Text 1\")\n",
        "\n",
        "![alt text](http://mathpullzone-8231.kxcdn.com/wp-content/uploads/thechainrule-image3.jpg \"Logo Title Text 1\")\n",
        "\n",
        "![alt text](https://i0.wp.com/www.mathbootcamps.com/wp-content/uploads/thechainrule-image1.jpg?w=900 \"Logo Title Text 1\")\n"
      ]
    },
    {
      "metadata": {
        "id": "YXORgiKIFzTw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def lossFun(inputs, targets, hprev):\n",
        "  \"\"\"                                                                                                                                                                                         \n",
        "  inputs,targets are both list of integers.                                                                                                                                                   \n",
        "  hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
        "  returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
        "  \"\"\"\n",
        "  #store our inputs, hidden states, outputs, and probability values\n",
        "  xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
        "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
        "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
        "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
        "    # to calculate the hidden state at t = 0\n",
        "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
        "    # ps will take the ys and convert them to normalized probab for chars\n",
        "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
        "    # -1 as  a list index would wrap around to the final element\n",
        "  xs, hs, ys, ps = {}, {}, {}, {}\n",
        "  #init with previous hidden state\n",
        "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
        "    # We don't want hs[-1] to automatically change if hprev is changed\n",
        "  hs[-1] = np.copy(hprev)\n",
        "  #init loss as 0\n",
        "  loss = 0\n",
        "  # forward pass        \n",
        "  #print \"inputs\",len(inputs)\n",
        "  for t in xrange(len(inputs)):\n",
        "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
        "    xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
        "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
        "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
        "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
        "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)   \n",
        "  #print xs\n",
        "  # backward pass: compute gradients going backwards    \n",
        "  #initalize vectors for gradient values for each set of weights \n",
        "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "  dhnext = np.zeros_like(hs[0])\n",
        "  for t in reversed(xrange(len(inputs))):\n",
        "    #output probabilities\n",
        "    dy = np.copy(ps[t])\n",
        "    #derive our first gradient\n",
        "    dy[targets[t]] -= 1 # backprop into y  \n",
        "    #compute output gradient -  output times hidden states transpose\n",
        "    #When we apply the transpose weight matrix,  \n",
        "    #we can think intuitively of this as moving the error backward\n",
        "    #through the network, giving us some sort of measure of the error \n",
        "    #at the output of the lth layer. \n",
        "    #output gradient\n",
        "    dWhy += np.dot(dy, hs[t].T)\n",
        "    #derivative of output bias\n",
        "    dby += dy\n",
        "    #backpropagate!\n",
        "    dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
        "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
        "    dbh += dhraw #derivative of hidden bias\n",
        "    dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
        "    dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
        "    dhnext = np.dot(Whh.T, dhraw) \n",
        "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
        "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yz5WZkbIFzT0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create a sentence from the model"
      ]
    },
    {
      "metadata": {
        "id": "fvmxXQ4MFzT1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "dc482faf-35e5-4ebf-8745-0eea06497463"
      },
      "cell_type": "code",
      "source": [
        "#prediction, one full forward pass\n",
        "def sample(h, seed_ix, n):\n",
        "  \"\"\"                                                                                                                                                                                         \n",
        "  sample a sequence of integers from the model                                                                                                                                                \n",
        "  h is memory state, seed_ix is seed letter for first time step   \n",
        "  n is how many characters to predict\n",
        "  \"\"\"\n",
        "  #create vector\n",
        "  x = np.zeros((vocab_size, 1))\n",
        "  #customize it for our seed char\n",
        "  x[seed_ix] = 1\n",
        "  #list to store generated chars\n",
        "  ixes = []\n",
        "  #for as many characters as we want to generate\n",
        "  for t in xrange(n):\n",
        "    #a hidden state at a given time step is a function \n",
        "    #of the input at the same time step modified by a weight matrix \n",
        "    #added to the hidden state of the previous time step \n",
        "    #multiplied by its own hidden state to hidden state matrix.\n",
        "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
        "    #compute output (unnormalised)\n",
        "    y = np.dot(Why, h) + by\n",
        "    ## probabilities for next chars\n",
        "    p = np.exp(y) / np.sum(np.exp(y))\n",
        "    #pick one with the highest probability \n",
        "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
        "    #create a vector\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    #customize it for the predicted char\n",
        "    x[ix] = 1\n",
        "    #add it to the list\n",
        "    ixes.append(ix)\n",
        "\n",
        "  txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
        "  print '----\\n %s \\n----' % (txt, )\n",
        "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
        "#predict the 200 next characters given 'a'\n",
        "sample(hprev,char_to_ix['a'],200)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " xM`m\tLg]�-D�C..sO5/Y{9ln'i v\\T72 �W��v�#Kfp\\g?,nJlh`R\tN8,a�P�Wh^-B-Qyp@E0�b\n",
            "=E >p8ZE>�ETJMHo(�Lf6}Jj��� yT\n",
            "4�%d@'##/6p�+Vr'B9_?O/xr!:\"�K%\"#%;+X6BmT>3&v6M-c]5UWKx0mHgoW)W{]�#\\8�wf1S^~`_Q\\zPs1#0NT�MA�J\t \n",
            "----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L8unlsJvFzT-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Training\n",
        "\n",
        "This last part of the code is the main trainning loop:\n",
        "* Feed the network with portion of the file. Size of chunk is *seq_lengh*\n",
        "* Use the loss function to:\n",
        "  * Do forward pass to calculate all parameters for the model for a given input/output pairs\n",
        "  * Do backward pass to calculate all gradiens\n",
        "* Print a sentence from a random seed using the parameters of the network\n",
        "* Update the model using the Adaptative Gradien technique Adagrad"
      ]
    },
    {
      "metadata": {
        "id": "ydiGgVIGFzUD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Feed the loss function with inputs and targets\n",
        "\n",
        "We create two array of char from the data file,\n",
        "the targets one is shifted compare to the inputs one.\n",
        "\n",
        "For each char in the input array, the target array give the char that follows."
      ]
    },
    {
      "metadata": {
        "id": "72t7zRMgFzUF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "38e078c8-ac1a-4f6f-c23a-75b5b0e2be94"
      },
      "cell_type": "code",
      "source": [
        "p=0  \n",
        "inputs = [char_to_ix[ch] for ch in aa]\n",
        "chinputs = [ix_to_char[ch] for ch in inputs]\n",
        "print \"inputs\", inputs\n",
        "print chinputs\n",
        "\n",
        "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
        "print \"targets\", targets"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs [54, 18, 20, 49, 46, 108, 106, 104, 77, 24, 46, 22, 102, 50, 50, 52, 102, 78, 24, 105, 46, 22, 104, 110, 54, 18, 108, 50, 104, 23, 102, 46, 48, 102, 47, 104, 44, 106, 102, 78, 47, 105, 77, 93, 89, 105, 77, 101, 52, 47, 104, 49, 106, 104, 24, 78, 109, 110, 54, 18, 108, 50, 104, 23, 102, 46, 48, 102, 47, 104, 52, 21, 109, 23, 104, 78, 78, 104, 75, 110, 54, 18, 108, 50, 104, 23, 102, 46, 48, 102, 47, 104, 52, 46, 21, 102, 77, 47, 104, 23, 102, 47, 104, 110, 54, 18, 77, 104, 51, 46, 49, 106, 106, 102, 77, 20, 52, 18, 46, 51, 105, 20, 24, 21, 110, 52, 61, 106, 106, 110, 54, 18, 77, 104, 51, 46, 49, 106, 106, 102, 77, 20, 52, 18, 50, 104, 46, 24, 105, 49, 77, 50, 23, 102, 46, 104, 78, 110, 52, 61, 104, 106, 110, 54, 18, 74, 104, 47, 105, 77, 52, 20, 49, 46, 108, 106, 104, 77, 24, 110]\n",
            "['\\n', '\\\\', 'd', 'o', 'c', 'u', 'm', 'e', 'n', 't', 'c', 'l', 'a', 's', 's', '{', 'a', 'r', 't', 'i', 'c', 'l', 'e', '}', '\\n', '\\\\', 'u', 's', 'e', 'p', 'a', 'c', 'k', 'a', 'g', 'e', '[', 'm', 'a', 'r', 'g', 'i', 'n', '=', '1', 'i', 'n', ']', '{', 'g', 'e', 'o', 'm', 'e', 't', 'r', 'y', '}', '\\n', '\\\\', 'u', 's', 'e', 'p', 'a', 'c', 'k', 'a', 'g', 'e', '{', 'h', 'y', 'p', 'e', 'r', 'r', 'e', 'f', '}', '\\n', '\\\\', 'u', 's', 'e', 'p', 'a', 'c', 'k', 'a', 'g', 'e', '{', 'c', 'h', 'a', 'n', 'g', 'e', 'p', 'a', 'g', 'e', '}', '\\n', '\\\\', 'n', 'e', 'w', 'c', 'o', 'm', 'm', 'a', 'n', 'd', '{', '\\\\', 'c', 'w', 'i', 'd', 't', 'h', '}', '{', '2', 'm', 'm', '}', '\\n', '\\\\', 'n', 'e', 'w', 'c', 'o', 'm', 'm', 'a', 'n', 'd', '{', '\\\\', 's', 'e', 'c', 't', 'i', 'o', 'n', 's', 'p', 'a', 'c', 'e', 'r', '}', '{', '2', 'e', 'm', '}', '\\n', '\\\\', 'b', 'e', 'g', 'i', 'n', '{', 'd', 'o', 'c', 'u', 'm', 'e', 'n', 't', '}']\n",
            "targets [20, 49, 46, 108, 106, 104, 77, 24, 46, 22, 102, 50, 50, 44, 89, 6, 23, 24, 5, 22, 104, 24, 24, 104, 78, 23, 102, 23, 104, 78, 101, 52, 106, 49, 20, 104, 78, 77, 46, 79, 110, 54, 18, 106, 49, 20, 104, 78, 77, 46]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-YppmBtlX6lr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "aa=r\"\"\"\n",
        "\\documentclass{article}\n",
        "\\usepackage[margin=1in]{geometry}\n",
        "\\usepackage{hyperref}\n",
        "\\usepackage{changepage}\n",
        "\\newcommand{\\cwidth}{2mm}\n",
        "\\newcommand{\\sectionspacer}{2em}\n",
        "\\begin{document}\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i_oAKOPIXouu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "2a9819f3-5b54-4a41-afd8-487202f1deda"
      },
      "cell_type": "code",
      "source": [
        "sample(hprev,inputs,1000)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " ss[clargines\n",
            "$ Hin.2}{SBA CeadleskerPrellen's summel CSAN] '17}\n",
            "  \\item Pranel litepar lebright ang 2042]{}\n",
            "\\item 5, Ext[& @$tolala cingine*{\\textbf{Flly\n",
            "%%\n",
            "\\usels\n",
            "%\\itemertix:ER. Uhed sileeter bedigh{ictewor, EMECAE.,ntervigilys{AjableB]{+actionsiing s8fobige{commabiuging Imcals}{#ax{9�4}\n",
            "    {Develop{Veatemckicitep{gite}\n",
            "\n",
            "\\itelsege liche.come/gola, ICTEWTMicaitemizpace{0.65colizegingeliyze{\\texkbrET%H 2015}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%------ \\resueler $\\begincemancentabsen{2cmade Res:ad the coue}{Jun llinawion}\n",
            "\\begin{otesich, sole thee, angiskelection}{Cob Lesenin to\n",
            " {INNOIFd abl}{0.} FBifize} $\\begintedide CQU89\n",
            "%%%%%%%%%%%%%%%%%%%%%%\n",
            " \\\\begin{rebo wietipt}\n",
            "\n",
            "%\t%--------------------------LA5Laclane, niad}, HA� ABECm/PIENUJ6}\n",
            "\\pars: Hacl \\& Hertitionted and in Graslacolod, foware}{E, $\\larioftit[commanderieD 2010}\n",
            "\n",
            "%%\n",
            "\n",
            "\\item{\\\\slest the prosment}, PLC,ngacs bes\\li \n",
            "----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L_81uHOWFzUT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Adagrad to update the parameters\n",
        "\n",
        "This is a type of gradient descent strategy\n",
        "\n",
        "![alt text](http://www.logos.t.u-tokyo.ac.jp/~hassy/deep_learning/adagrad/adagrad2.png\n",
        " \"Logo Title Text 1\")\n",
        "\n",
        "\n",
        "\n",
        "step size = learning rate\n",
        "\n",
        "The easiest technics to update the parmeters of the model is this:\n",
        "\n",
        "```python\n",
        "param += dparam * step_size\n",
        "```\n",
        "Adagrad is a more efficient technique where the step_size are getting smaller during the training.\n",
        "\n",
        "It use a memory variable that grow over time:\n",
        "```python\n",
        "mem += dparam * dparam\n",
        "```\n",
        "and use it to calculate the step_size:\n",
        "```python\n",
        "step_size = 1./np.sqrt(mem + 1e-8)\n",
        "```\n",
        "In short:\n",
        "```python\n",
        "mem += dparam * dparam\n",
        "param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update \n",
        "```\n",
        "\n",
        "### Smooth_loss\n",
        "\n",
        "Smooth_loss doesn't play any role in the training.\n",
        "It is just a low pass filtered version of the loss:\n",
        "```python\n",
        "smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "```\n",
        "\n",
        "It is a way to average the loss on over the last iterations to better track the progress\n",
        "\n",
        "\n",
        "### So finally\n",
        "Here the code of the main loop that does both trainning and generating text from times to times:"
      ]
    },
    {
      "metadata": {
        "id": "bVB4VU1LFzUV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16759
        },
        "outputId": "b8d6347e-7005-4a1a-a221-5c602277eb79"
      },
      "cell_type": "code",
      "source": [
        "n, p = 0, 0\n",
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
        "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
        "while n<=1000*1000:\n",
        "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
        "  # check \"How to feed the loss function to see how this part works\n",
        "  if p+seq_length+1 >= len(data) or n == 0:\n",
        "    hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
        "    p = 0 # go from start of data                                                                                                                                                             \n",
        "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
        "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
        "\n",
        "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
        "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "  # sample from the model now and then                                                                                                                                                        \n",
        "  if n % 1000 == 0:\n",
        "    print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
        "    sample(hprev, inputs[0], 200)\n",
        "\n",
        "  # perform parameter update with Adagrad                                                                                                                                                     \n",
        "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
        "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
        "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "    mem += dparam * dparam\n",
        "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
        "\n",
        "  p += seq_length # move data pointer                                                                                                                                                         \n",
        "  n += 1 # iteration counter    "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, loss: 235.421236\n",
            "----\n",
            " i  on gyi,ccX`DSWtvs islo  f OcaC o rCkrin{eeuleSch|gSo/uqn\n",
            "en}r 1   r M2AGt a  -euo r hoce}c 0o osoo  e\\rn rCoru\\sdt ecrre}yns fiyenam rmeofe akPr{v crs,cd E}o redbtrollat-clet  u \n",
            "  n  te{iarp, ecbe \n",
            "----\n",
            "iter 1000, loss: 229.939310\n",
            "----\n",
            " oeuv i s sue   n eunt m rm\\ i umibnoclic cNs Gteeu mts dnimhoadc iilfte tnea ie f\\iute lA i at tpusRe  ienS a\t  e as \n",
            "r\n",
            "eoct te lyum{\tsaa m emt\tnseto d m id iuui n{e\tSmu-n   reBalthA  teisoP\\0Jl- eerp \n",
            "----\n",
            "iter 2000, loss: 200.413830\n",
            "----\n",
            " cpd\n",
            " tdsCe seistpMuA yc stnsrxrpteo2Nehdotm ibm    sde  o\\ ls  is\n",
            "\n",
            "cen mt\n",
            "rr\\i edu iu\n",
            "e \tpsu p o- {lomr y   nloed m\n",
            "} +b0k\n",
            " el pd e0itdvyaN} retdm0oorrustroe.g.dnn)tPrd\n",
            "ob pscdteeth iemE\n",
            " ri l\t dr0esn \n",
            "----\n",
            "iter 3000, loss: 183.059735\n",
            "----\n",
            " f.m a me da  on\n",
            "ltamlyAl ontere.peduis \\un. lIo. aa \\ M.Sped\n",
            ".  ocunimchopsomB  N \\sinainnrless}rof   ant  sh  \n",
            "\\ :SZtmIgdhoheolrupamasf{TerT -ecnmpe  awi  rcan =   onvei coe   o nroluanu{%s/iuihe  4  \n",
            "----\n",
            "iter 4000, loss: 165.964281\n",
            "----\n",
            " e` SAVirl V%tothhont{\\hushothgl \\sttbotur {0Ht0r Pthocp Hexstocoms{fol.\t\\ A L \\ %haPiotobt{ {htt}5/mtentt`1`ed 2 d2a1thiotie.3aror\n",
            "th %-Xf{wilth\n",
            "  onttfitCLpertkah Loft 1w1-N 3Itht,fomK/Pobl ocomm1/La \n",
            "----\n",
            "iter 5000, loss: 152.177064\n",
            "----\n",
            " emeu.an  dn1t}\n",
            "Sen \n",
            "\n",
            "\\n{\\uresem 7 P cin}{petecagegemave\n",
            "\\sin\n",
            "Sinozeminenelemer ,nsen{esk\n",
            "\\memon  lal, \\.Hyn}t..0.\n",
            "}\n",
            "\n",
            "\\lotemen Pp{e\n",
            "\n",
            "\\.onses{a \n",
            "\n",
            " fonuredo\n",
            "\\rfpi.}J\\\n",
            "\t\\len fe{5\n",
            "7arm\n",
            "\n",
            "\n",
            "\\mef T\n",
            "crale0.\t\\ti \n",
            "----\n",
            "iter 6000, loss: 145.351312\n",
            "----\n",
            " kewolinmesen.f--cll \\-Las c 10=f In} Dad \\an .08 Nodlt/rps/at2w}\n",
            "\\hmm-.stl5clkfy E:Glwfmavy loder JwIkL w Ms MSt .0.manetk08\n",
            "\t\t\\hik, V \\ilt  \\ Wfmerupritextlarkan \\SensuaH   craze{& \t\t\t\tole{T)}\n",
            "X6egur \n",
            "----\n",
            "iter 7000, loss: 138.140693\n",
            "----\n",
            " ngeItest=.91NIlenkergefenolhlude[tidith\n",
            "\n",
            " s.}[=.aligteMy Sysheddwerterke=thitmochuve}[ferdHis, Crh=miletitri}[prerectmintpiz={bint, \\rathinirt./Sbark] leiuntinkastit,\n",
            "6 (7svhuioreteg=/Litheblwitrerhem \n",
            "----\n",
            "iter 8000, loss: 134.989956\n",
            "----\n",
            " medwof  QurledElkuhlaxpeme loutirf tod wonqeorsthusubEProrholivaotilkerewusine{e P At 2edommJomaseanFtaseqth    \\tsenser, lem\n",
            ") u Laghinxs 20ndroxtirgemmeor (+age % HefrapeswA  anityonIps  wubendsegem \n",
            "----\n",
            "iter 9000, loss: 130.174346\n",
            "----\n",
            " Bcll#aROSO wentaopy St OSSpat, Jfigeli ctagale DO6w dkS0}\n",
            "| P SfaVivecurcy salSingotk.2  Ilkore  \\uaSurFstbujosssy}\n",
            "            \\apg RvOTamstycar wyzem ang Wafincthotla So)}\\ usagrorwkas +snactiosloJN \n",
            "----\n",
            "iter 10000, loss: 128.402719\n",
            "----\n",
            " olocarr Cempladeconsicstinins, \n",
            "  sexe tres Andelecoedrraendruand E; PontendssomIad - se deltorcu\n",
            " ci gesty Suntinyorocale.\n",
            "   -     esumeOtindcomgrs, edort titgorn wdend AContemssecHPolace Ecd arlera \n",
            "----\n",
            "iter 11000, loss: 125.474852\n",
            "----\n",
            " psice2end tcamctlerlectand som\n",
            "v\n",
            "\\stres Comt{\\itetor\n",
            "\\it w cabr UMS-fostbs antond wof .conerk}\n",
            " {AOJal onk lobtaneruagr}\\beg{Mageco ang rentoghuntithfa, \n",
            "    Pactem SfincoAd{isitbring ATuonS\n",
            "    {\\ed  \n",
            "----\n",
            "iter 12000, loss: 125.330550\n",
            "----\n",
            " eroute AN2inchisepion*{\\acetided seBtadtciextbegsurapoge \\temecaly\n",
            " Jore}tl ingw\n",
            "\\engripedesistalcalonyctize ofatele}\n",
            "\\ ortinaleduh. cofont}\n",
            "\\nse.  Af{nome[\\seachiopinenoce{Rinpe\n",
            "\n",
            "%6stitete{cem{%%%\n",
            "\\h \n",
            "----\n",
            "iter 13000, loss: 122.790947\n",
            "----\n",
            " uedouddacated\n",
            "\\bed{dentalyckadfacsseptovite\n",
            "\\etlratatantomathasect,pofad{ayedatadrems\n",
            "\\hteacast]{bf{thlpaventomaockade}}} andumaterdadum\n",
            "8]tepem}\n",
            "\n",
            "\\edam 2}}}\n",
            "\\be{\\tectrewcam}{adth\n",
            "\n",
            "\n",
            "Cingy[t,\n",
            "\n",
            "\n",
            "Fand an \n",
            "----\n",
            "iter 14000, loss: 121.168877\n",
            "----\n",
            "     \\\\ enging 2 \\stem Ltinxtalas{'S detion}\n",
            "\\etome int tecem tm}{\\pectentmepcool and wurling}\n",
            "\t\t\\texth  honthraf{irce pentricl} fedry 2017.cae}\n",
            "%\\dedaw stbjeslece[.5] fe\\textbf{inperilobasueticl C.50. \n",
            "----\n",
            "iter 15000, loss: 119.559790\n",
            "----\n",
            " sytlit{Rloolad - SancemoParn Xrachst, imeds) Ctbavalr of a}}\n",
            "     /itt}\n",
            "\\slater utem alline lats, tdrub Cdulem ardy st cy iubut. \\eod aving sonssut\n",
            " \\hantebtafaly sytralebtin (al buatlows\n",
            "        \\t\n",
            "( \n",
            "----\n",
            "iter 16000, loss: 116.647841\n",
            "----\n",
            " legem\n",
            "\t\t\\ss\n",
            "\t\t\t\t\t\t\t\t\t\t\n",
            "\t\t\\in \\ile{kEGGericetermarl 2omerlerseatitem\n",
            "\t\t\t\t\t\t\t(\t\t\t\t\t\\beghect:;\\mina\n",
            "\t\t\t\t\t\\endemare}\n",
            "\t\t\t\t\t3\t\t\\rlatha:qithinemm{\n",
            "\t\t\t\t\t\t\t\t\\ragrrik\n",
            "\\begin{he\\ll\\lepre}\n",
            "\t\\englherihew}\n",
            "\t\t\t\t\t\t\t\t \n",
            "----\n",
            "iter 17000, loss: 115.556806\n",
            "----\n",
            " pann cesThionpadledreacandion, Dhgl]{Pane yse 2017 \\\\{Locd Glico CSubsigwidtlt, KoItcofolt\n",
            "\n",
            "  Mondizendatsod 2004 In Ocogficyic tateot#agrion}2 R@B Coagi A angmiop}{Sectirl Ghomize{SA llletand t Exest \n",
            "----\n",
            "iter 18000, loss: 113.668706\n",
            "----\n",
            " }}{5ip]}\n",
            "\\plogt tEm, \\heth}\n",
            "  \\new (16\\bf{hthle}{L99 (---------- \\sextbfion{--------- \\seataen, \\rtawh=009927} ty]{\\textw.pro); 2015setht=1] an)}{w.50.5---------------------------b.[]{\\copetyctarke  \\ \n",
            "----\n",
            "iter 19000, loss: 110.999425\n",
            "----\n",
            " ost arer and Cyftbrrili Bherek, (Rinjet \\hfila caming thspaderl jend (daal{:5 here rfans}\\\\edidten te}\n",
            "\n",
            "       \\rzewing}}\n",
            "\n",
            "\t\\resutivelony 200m}\n",
            "  \t  \\hendeno. Dhitite te{stildicke Inipnind{lenum Engin \n",
            "----\n",
            "iter 20000, loss: 112.322615\n",
            "----\n",
            " n OH soves (Sub ang Tegiten 6b Sery centhy \\s secsL Omemat}{Javev:.p}{\\veguiser srulerargor 1}  Drige}  \\uspehtiongal se: Adver endros }  uvelor }\n",
            "     \\tepummeng An - ANrlinen ty to taP ED Apeckis em \n",
            "----\n",
            "iter 21000, loss: 110.298899\n",
            "----\n",
            " padl, Ph\n",
            "  \\iss\n",
            "\n",
            "\t\\ilemi0lar whice. +.RDin Pe Hhe gyctescir #00009 8.RE%%%%%%%%%%%%%%%%%%%%Dentby\n",
            "\t\\ Lenchoiowcos, Giniblin, End Malhingen \\ecpem Scacy cubfeusuliydericce puavizim-EngEPa.}\\nalage Eni, \n",
            "----\n",
            "iter 22000, loss: 110.107264\n",
            "----\n",
            " IlICITIP)\n",
            "\t%\n",
            "\t{ho.med\n",
            "    {B+22.83 utepsor, CrSSemermences ins.\n",
            "   {pradshcomend 1.grorte \\ \n",
            "\\ressevee, Larts\n",
            "%%%%---------------------\n",
            "\\beg }\n",
            "  Bted{ICFpt\n",
            "% SLA ILerine{Jate}}\n",
            "  {\\\n",
            " {  PXm01� Ty BSKa \n",
            "----\n",
            "iter 23000, loss: 108.848780\n",
            "----\n",
            " ate. Putingemdrlan} RmKD:DMongrte hfod inton 20malmize}s af Lbutt Ingycte Dede Lare 10.5s}\n",
            "               Kersion*cradur D meI\\hrad{*01260mall of driummureicitem \\toorn cytiontedingmite sh peon caspor \n",
            "----\n",
            "iter 24000, loss: 109.258370\n",
            "----\n",
            " ler } mcr13\\bogemmeragracs in proumas angoliol rmaty sh wcalemeSpbolol:/Ejh{Wobtoat}\n",
            "\\erblemmenchlors.|.\\blad LAp:3)\n",
            "%\\begin{al}\\stedlemanced in}\n",
            "\\sepraal bersiond CANMcuphmeScattuafri Masted ruticod{ \n",
            "----\n",
            "iter 25000, loss: 106.943552\n",
            "----\n",
            " -I}\n",
            " \n",
            "\\ont\n",
            "\n",
            "\t\n",
            "\\sspacy\n",
            "\\rendenzengen{uchlormand{tion{'afam ovs}\\tant, \\textbf{Vnolting\n",
            "\n",
            "\n",
            "\n",
            "3}[d,,Temexpadtiluc}\n",
            "  \\iteacuctedin ftitwof{Memepin)*\n",
            "\n",
            "\t\\usubintto!betertumath tuth:/bltcradupmarst \\hextbunue \n",
            "----\n",
            "iter 26000, loss: 108.092965\n",
            "----\n",
            " rons 1nt}\n",
            "                    \\usle acolicd So thent.7}\n",
            "\t\\beginpmm}\n",
            "\t\t\\item Raliin and thize}{pins /arze}: Roccatof slto chictuakicdin..}%%% \\\\\\setcovbectiow}}\n",
            "\t\t\\inench Shigy//ar acts\n",
            "  \t\\iteaduaval  \n",
            "----\n",
            "iter 27000, loss: 105.759547\n",
            "----\n",
            " \n",
            "\\end{Spaget\n",
            "\t\t\\usepickagkcck33375phreade Ł5}%            engpice(}}}\n",
            " &    \\sedgen, Gwize}  l}\n",
            "\\hextbfilkcer}\n",
            "\t\\begin{(cobubergL.353]}\n",
            "\t\tRebfeludgvak}}\n",
            "\t\t\\hnuligele[riinion, (culared Kropn{plont}}\n",
            "\n",
            " \n",
            "----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "iter 28000, loss: 105.487566\n",
            "----\n",
            " evelahub \\\\\n",
            "\\endicteangroferedsy ---------------EACTward conill tevelog\n",
            "\\inesedy linaliting licrledilgiterdection, er raage FEmize 0126PFA) to Meicepackadeer Id tectiegted Ap\n",
            "\\& \\lext wicumadipe filac \n",
            "----\n",
            "iter 29000, loss: 104.560240\n",
            "----\n",
            " ein{2016 Roorger  Devjecie hrong SOS/ion{2017 Grind lyiveb Comx\n",
            "$500} of 2016/Sip.}\n",
            "        Wontor Teb masiyve}\n",
            "    & Wable orsits, Soroine}{KA/CION \\rsy{CublewiN100}{Qub rof ment}\n",
            "    \\item Pheck Dat \n",
            "----\n",
            "iter 30000, loss: 103.230375\n",
            "----\n",
            " -------------------------------------------------------------bulub of }\n",
            "  \t (----------------- O/, W, Ionicrs}\n",
            "\n",
            "\t\t\\}\n",
            "\n",
            "%_914 Py theng Shubr recsin }\n",
            "                         \n",
            "\t                          \n",
            "----\n",
            "iter 31000, loss: 103.847811\n",
            "----\n",
            " Sumavbubsict\\begin{ientsinh PbHe Seb ot}\\mted thter} &-2}}\n",
            "\\noft\n",
            "% Gptlathrevb}\n",
            "\\veeccle}[ping{\\texttutilt Suly} (3p}\n",
            "\\bahartearate ind Act on Ixt--{tolecolsinter fo lubgreingl \n",
            "\\usepack}{\n",
            "\n",
            "\\ctedion:  \n",
            "----\n",
            "iter 32000, loss: 103.317772\n",
            "----\n",
            " nd eng.}\n",
            "\t\\hendesnensiteStmerertaris test{Ind li, Augemment w  \\hrase int E1017* 3\\tina to Seginch fraf Aceimate hjextrding cing in[e}\n",
            "\\end{tectiuk: Kh Shopavelerecdrorgewiver oris plobing\n",
            "%\t\t\t\t   \\nu \n",
            "----\n",
            "iter 33000, loss: 103.291075\n",
            "----\n",
            " }{\\item Endine}}\n",
            "      \\begin{}\n",
            "      \\item{Maren}\n",
            "      \\begin{radtivit9}\\\n",
            "      \\rBsumeSul}{3 REO]{&CA}\n",
            "\t   Axtulave{2cimiteded sewil dera-- oradive{SOES.}\n",
            "\n",
            "%----------28}{\\item Cor ulovegh.congit\n",
            "\\ \n",
            "----\n",
            "iter 34000, loss: 102.856212\n",
            "----\n",
            " s]{dfill bo \\hrgintermy\n",
            "\\end{rm \\begin{lasendind \\hyvald \\resumeSdempmarg\n",
            "  \\lssatenonte}{Table\n",
            "\n",
            "\t\\item\n",
            "\\sept}{mind{pralnaradadlhstadadnast\n",
            "\\textwitslvghe{1011/Rsirgh:}{ling{utedumetedtord}\n",
            "  \\cew{wof \n",
            "----\n",
            "iter 35000, loss: 103.780995\n",
            "----\n",
            " em Propererendedin .31014) sscrader pm0erge) spubiecomememen ryseform} WeatimpicytquangHe deth Jan vipalojes] & Atalle}{Gintor laf Af*enkine rfySubfer 2xmemize}}\n",
            "  \\riffub.( Trove)pion}\n",
            "  %\n",
            "  \n",
            "://gima \n",
            "----\n",
            "iter 36000, loss: 101.177546\n",
            "----\n",
            " hpebkerke.%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  \\reskerasuled \n",
            "\t\t\t\\eve,}=.\n",
            "\t\t\\edr. \\& \\textbf{Dje.agemM| arning & J]{m, E (Lingentidth}\n",
            "\t\t\t\t\\item Alaneme \\resubemenilage & Cebrefthras} to Werge fort tearet \n",
            "----\n",
            "iter 37000, loss: 102.333058\n",
            "----\n",
            " ATSk},�%%%%%%%%%%%%%%\n",
            "\\begid{htubucolSpertonWansedth}\\& Peef rosppectilize{}\\href{Biont ’/ouibSecventicofize Injeapegif{balo ast-0yJent-14}\n",
            "\\bf Uppofoctewion, Fomport seriansexttul Colul Bath tontia \n",
            "----\n",
            "iter 38000, loss: 100.910307\n",
            "----\n",
            " eassils amath sabusum Unast}{----------------\n",
            "\n",
            " & Ion{-4editt mpesk\n",
            "  \\item Apselr searys, Ege}\n",
            "}. MLU8]{-\n",
            "  \n",
            "\n",
            "\n",
            "\\textsudreruminch aragem{WobHontom, SER Jeveternury)}{\\maze{Iune of sesumart, sedursigin \n",
            "----\n",
            "iter 39000, loss: 99.763208\n",
            "----\n",
            " DPHEICN(-\n",
            "\\begip{dof vorFar}}{SUMABUSFival and; \\nAld }\n",
            "\\liygrt Puigl, TT, Jack, IDE1nglor, uparerseat, Mas pars +.57%  \\resubs\n",
            "\t\\resumentt, bis, IVepabul,/Callatin, svuccolSugisting, \\textrminc, DV)} \n",
            "----\n",
            "iter 40000, loss: 100.885045\n",
            "----\n",
            " usecteation} 32\\manar dith an pisr litho sickoctw, ind section}{----s,-000ss quers}, uteges ssegerse BarinQectt an $ G Fere, calpageckarulrat/sarced ensublopseclizel Wole}{Dh: Gas thoor codon{2 rSeen{ \n",
            "----\n",
            "iter 41000, loss: 99.284275\n",
            "----\n",
            " ]}\n",
            "   Debsh{Cedesion}}\n",
            "\n",
            "\n",
            "  \\item\n",
            "  2004ictercr - D and Sobshinth\n",
            "\n",
            "%\\hrobutt ------\n",
            "  {\\s}}\n",
            "\t{\\chulinterngvictar a. ------------ andLiteengwagy Shicupar icinizo}\n",
            "\t\\item Itso}\n",
            "\t\\se{taidiogwiteliclas=0.4 \n",
            "----\n",
            "iter 42000, loss: 99.622652\n",
            "----\n",
            " t \\resumewvelcaw orgiteSugen{cofrer tebysile 30}}\n",
            "\\end{rassidth mpuster suterdon{1cheding[tect, Eid{mbinPerne seth s.--MEN EDPLaclol s.RENMocumet/VSecter\n",
            "\t\\begin pamemmivervectcin{roiknef urazementhan \n",
            "----\n",
            "iter 43000, loss: 99.226488\n",
            "----\n",
            " ser}; fformaresealy angwor Barter conter cale.\n",
            "     {CAU\\%%%      cimes}}\n",
            "\n",
            "\n",
            "  \\seller Jave iraule, Calalass dest ’chodleats matice\n",
            "\n",
            "%A\\meSubuestLK}\n",
            "}{Toront c AptL, sy--Pracge inersic and{Ffos ofoat \n",
            "----\n",
            "iter 44000, loss: 100.668772\n",
            "----\n",
            " seluge DoCSperies firt acutiniptenterthit\n",
            "\\item Uoupedine ball thyth mopp{ming U Te bwacynenseioips }{Pallas 2018//gemmill act chol{come codthlidectoom{\n",
            "\\rend debl }{Axthizet and and teplen,kin'l \n",
            "  \\ \n",
            "----\n",
            "iter 45000, loss: 98.569761\n",
            "----\n",
            " skilinimyPlatinn}}\n",
            "\t\t\t\\teplitle arahensection}%\n",
            "  \\item Linitemmall enillor doning (Coftat.com}{Ziniulmination{Mall Sulkie stiversive}\n",
            "\n",
            "\n",
            "\n",
            "\\section{2014 Pitim 2incho}\n",
            "      \\itemigementno�tiinted Mang  \n",
            "----\n",
            "iter 46000, loss: 100.541110\n",
            "----\n",
            " sedecor}.|}{7ont.\n",
            "  \\item 2em}{\\textsu{resent}{Dlo} Decterms} oudect}{00]{Bomm{-itemth}\n",
            "\t\\item{0}}\n",
            "\n",
            "\\end{inemsmazepnofertment.}\n",
            "\n",
            "\t\\sect#}\n",
            "           \\begin{ramessitem}{\\sogneommancERestitextems://gite \n",
            "----\n",
            "iter 47000, loss: 97.946342\n",
            "----\n",
            " \n",
            "\t\t\t\t\t\t\t\t\\endlotkzentseylite}\\-\n",
            "\t\t\t\\ister};\n",
            "%\t\\begin{mamsh{\n",
            "\t\t\\seabe}\n",
            "\t\t\t\t\t\t\\seck pthing\n",
            "\t\t\t\\itse[\\href{holpalle (JAUsignt., and Anage{Sorke}\n",
            "\t\t\t\\end{minill bik be wet, Javax:pin } \n",
            "\t\t\t\t\t\t\\velly,\\\n",
            "\t\t\t \n",
            "----\n",
            "iter 48000, loss: 98.973657\n",
            "----\n",
            " lor}{\\p{SebfemeNing{#4016}{}%%%%%%%%%%%%%%%%%%%%%%E}{\\bf{(25016 14*#45}}\\blacengo scrasy{PADIldibulort ol Teacottor*} \\siclosst (ADG theanions lathalad thol]{mill.\n",
            "\t-   \\end{#/0.0,mp{tonew�__ibs #comu \n",
            "----\n",
            "iter 49000, loss: 98.154338\n",
            "----\n",
            " }\n",
            "\\\\\n",
            "\\emfet.\n",
            "\\item{Dougement\n",
            "\n",
            "  \n",
            " \\beginlebfore}}\n",
            "\\useps\n",
            "\n",
            "\\sigging}\\small}\n",
            "\n",
            "\n",
            "\\itemiets bsewale etiagSlingeting\n",
            "%r\\inger}\n",
            " {Pd}(mall{inghalill}\n",
            "\\headers:/4ang: Suged}}\\\n",
            "\\usecolceSted fancad proma};\n",
            "\\te \n",
            "----\n",
            "iter 50000, loss: 96.035860\n",
            "----\n",
            " yltom pid toftrminertoxubreent}\n",
            "  \\item Bows]\n",
            "  \\eres}}}\n",
            "%---------/idicgh, ICcoLASAlecllester th=6----------------------------------------------------------------------------------------------------- \n",
            "----\n",
            "iter 51000, loss: 98.272342\n",
            "----\n",
            " scom CmfassLitimszatthork\n",
            "%\n",
            "\\begin{sloplitslocoat dotericuters\\& commats.}\n",
            "\\begin.scersictliped threm SubSubfalls\n",
            "\\vsectill tuble 2006 & & coppitem stingrrecalock\n",
            "Bathritrugrtbelel ful 2015 & \\hfillti \n",
            "----\n",
            "iter 52000, loss: 96.910158\n",
            "----\n",
            " adiog ffoiling\n",
            "\t\t\t%%%\t\t\\taule\\sebHewcis}\n",
            "\t\t{\\emylormemize}\n",
            "\t\t{\\tasulation%\n",
            "\t{0.4TSO+milinesthtirenermysion inary in ind{-------2ction ofilce 1.Prorkhines and Matipler}.\n",
            "      \\itemize}\n",
            "\n",
            "%\n",
            "%%%%%%%\n",
            "\t\t\\e \n",
            "----\n",
            "iter 53000, loss: 97.280583\n",
            "----\n",
            " ende leolol ec@ced dask\n",
            "*filceche '18269-4LU}\n",
            "\n",
            "\n",
            "      & -----------------------------------MON Cofeb seice pabecory sectrectrorler per}\n",
            " .\\resumeSler@on}  fol Seahed ang anding ACSSEA CrupeSleng thech \n",
            "----\n",
            "iter 54000, loss: 96.312169\n",
            "----\n",
            " ing Ale2 \n",
            "    \\itllaling.};\n",
            "  {\\uvats]{rch}{\n",
            "\\itencemLedras=ill ient \\hendeate anding in vilage[LA Te3Xarditg uane}}\n",
            "       \\natl}{\\textwit}\n",
            "\\upednead tsem.\n",
            " %                \\itee duge[narh}\n",
            "\\end{Iss \n",
            "----\n",
            "iter 55000, loss: 98.172235\n",
            "----\n",
            " n Coranf{Thens bane. ISTSPSECSOciction}{\\lass Seadnh\n",
            "\\hagd{Projom]{Iffor Sior({EPminakentfor and batinn}\n",
            "\n",
            "    %% $0.}{Angem{I foly Apege '19 MA Coner and inar weadche sangatlor\n",
            "\t\\section, Chul ahart a \n",
            "----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "iter 56000, loss: 95.394757\n",
            "----\n",
            " n*{Dendetcinch=Stikh[x} \n",
            "\\textbf{Dys\n",
            " \\& Fudersidth}{0D://blt\\\n",
            "\t\\hfill{{tounch}\n",
            " %\n",
            "\n",
            "\\inculequbuding \\vptom cting{\\textbf{hiptateofize}\n",
            "\\textwiont wath}\n",
            "\\begin{rsits}\n",
            "\\sectitHestemalo \\textst{3 \\: CTMG \n",
            "----\n",
            "iter 57000, loss: 97.784986\n",
            "----\n",
            " cr}{lakl}\n",
            "\t\\se[h}\n",
            "    \\hrall com Phy\n",
            "%\t\t\\texmend  hangerte{ \\vspraage{adotist\n",
            "}\n",
            "\n",
            "\\item[\\\\st. \\section: Hy/gat/?umon}{Susin}{\\textwize. DL..dep}}}\n",
            "% corg 'paze}[r/DIENm}}\n",
            "\\begin{iludses}{SillotorSojf}  \n",
            "----\n",
            "iter 58000, loss: 95.396652\n",
            "----\n",
            " \n",
            "        {Imem ofin#}}\n",
            "      Segisst aviniph{}\n",
            "      {$\\mect 2011#2{SRIL8}\n",
            "\t\t{\\resumeSt{reston{}\n",
            "\n",
            "      \\begin{tLicuss props}}\n",
            "\\sesler  of Lograind in purvuh sucotion mpW and thEC2}, Marinsa ITQrginsh \n",
            "----\n",
            "iter 59000, loss: 95.920364\n",
            "----\n",
            " in, Walle \\ndol. $ ol Fore ex -1}, Sustoctalingricho Dela {do Sht torring\n",
            "\\ app  \\resumer, \\href{htextcerea lomWit{\\ecionshinaiew and/ealaro}\n",
            "\\heapectent, \\textwiont Wol and Eng, sinuor noares, simpar \n",
            "----\n",
            "iter 60000, loss: 96.024360\n",
            "----\n",
            "                                            nase Solincr Suct, Rusurib.  \\        Git Dechy usigh andAbiewcingel Praake, wSeworse} gesumetare congeb of Bagw rasctodvuage to sologn + ofivectinille br+,  \n",
            "----\n",
            "iter 61000, loss: 93.697928\n",
            "----\n",
            " 10m Prauh{huing, Am Exe}\n",
            "\n",
            "  \\hesupen} an Gis},\n",
            "\\vspace{}\n",
            "\n",
            "  \\rend{-------------------------------------------------------------------------------------------------------------------------------------- \n",
            "----\n",
            "iter 62000, loss: 95.105804\n",
            "----\n",
            " trableionters my a in Che a seduconilys MaverEmptomponk re--00 ICTOB0, Re Rearathe pans: Augungingpage de, walle cod sectuabubItem{SuWel Sare of the ta approring DA atsed mariege}\n",
            "      \\item Leadinhe \n",
            "----\n",
            "iter 63000, loss: 95.055553\n",
            "----\n",
            " %%- Deted cangen}}\n",
            "%    \\utets\n",
            "       Sew}{}%%\n",
            "%}]\n",
            "% hrensipth\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% DA} Datchy};\n",
            "%C\"%  Many/geteacoring puak us/engen}{8}%%\n",
            "         \\tepdind refid  \n",
            "----\n",
            "iter 64000, loss: 95.946934\n",
            "----\n",
            " jecrorress\n",
            "%Dative anst fors, – U\\%\n",
            "        \\vspace{-2)}{5ixsbol and 200/Antommovence ond ornichork foquion}\n",
            "  \n",
            "%USMLens}\n",
            "   \\lind Sersss stextbitage subfett Bhang and Wsum Apd andE}\\tmilysermem'l.\n",
            " \n",
            "----\n",
            "iter 65000, loss: 94.395028\n",
            "----\n",
            " \\me\\lul weang\n",
            "\t\n",
            "  finn\\ 1\\}]{Thrim{Incestthend ~penialeftw 1ence Fhmesecting thilfrerror, V/renght dyliticr') Ukernt) Wode Eptexout\n",
            "\t\\vspace}\n",
            "\n",
            "  \\\\}\n",
            "\t\\yse{Densh Dete Comerescemexptcharn Systemenciotut \n",
            "----\n",
            "iter 66000, loss: 96.186632\n",
            "----\n",
            " ve anMer/ 2pmarg enres (merl ad call*{Sup.com}{#16 I pite reqularo}{---------------------SECS4}\n",
            "\\item Ninchis\n",
            "% MES by 2017 - Ratlo \\textsc{Readr and - Maren{: Cutiguliet ation Decdempent, 3XI Commova \n",
            "----\n",
            "iter 67000, loss: 93.422758\n",
            "----\n",
            " , in initilickage[mige, lom=Prox}\\\n",
            "\t\t\t\\end{rsbulen indpedinshsenfork \\\\\n",
            "\t\t\t\t\t\t\t\\item Min}\n",
            "\\begin{Panginteriph{ilapadedions ARA Cot \\buginemmexkr@ Unice Deticlap{\\resumes:dicts obo}\n",
            "\t\t\t\t\t\t\t\t\\item, cobo \n",
            "----\n",
            "iter 68000, loss: 95.120773\n",
            "----\n",
            " temize}\\smallation, $\\-bate qragy}\n",
            "\\href{ltarty cor.carectitbItem ECER C and, ATSSL}}\n",
            "    {\\\\\n",
            "    {\\rogbP Rett Coblite DeveloD, Bardind grabuleshontes, silar for an 2016 ACS}\n",
            "\\items camane L1488}\\%%%% \n",
            "----\n",
            "iter 69000, loss: 93.948767\n",
            "----\n",
            " tlah=,qurat,=-65)}\n",
            "%n=4crank}\n",
            "\\usepackage[exp=.5y/Re=2.+2b+Opelini, mstkestbebchor=.cut]{tegnilicri,mphorkind thoutHuizes oreninHead =015co, Antentan, tetiont, rvulecommond);\n",
            "  \\end{mins\n",
            "%      \\resen \n",
            "----\n",
            "iter 70000, loss: 92.247874\n",
            "----\n",
            " ing\n",
            "\n",
            "\\inenitsmSDevelote}\n",
            "  \\hrolI tjepliced 05}\n",
            "     wate}{gdtleb in Gltitipt{\\_m/lessh plast chinIterskeadnt EnglO700086}\n",
            "\\begin{cseariancearesightListSubSterfiter\n",
            "\\ett{pt}\\htees sowes send{CS) KARE} \n",
            "----\n",
            "iter 71000, loss: 95.148013\n",
            "----\n",
            " Tevelrmontpicrio pobsidm.cyion}[sskraizag}{{Mons Sulecow, CT/MReniat{#pionel, Desto, Manege \\textsc{Engical E}\n",
            "\n",
            "\\vepusents}\n",
            "\\item{Wickle powidth{Engw. Te,ntom{AMG Sed and anC\" for'cer/ankict{119SSETe  \n",
            "----\n",
            "iter 72000, loss: 92.623403\n",
            "----\n",
            " nion The.\n",
            "\t\t\t    \\raggenverrerseredusce in Sheaf sumentaient\n",
            "\t\\hres eming of puverittoviveriena\n",
            "%\n",
            "\t\\usepade= OwOEI% repe}\n",
            "\t\t\\resumeStemevelsshuwien\\ fingyth wermize -\n",
            "%%\n",
            "%%%%%%%%%%%%%%%%%\t\t\\item Deari \n",
            "----\n",
            "iter 73000, loss: 93.876043\n",
            "----\n",
            " Cabys an A?{ho ple oul Xware, C9N, MDOJEXchentarging 2018 PrSpell Lamaglace hel  lit.io and and Ar---0.J20.5em}\n",
            "           {Dman Tror Unal 78---mentaation 2017\n",
            "%%\n",
            "         \\Hramals wade Reseat{Firchbu \n",
            "----\n",
            "iter 74000, loss: 92.621846\n",
            "----\n",
            " rt rowinaulero omertseras, testatraknyience}{Prockhctith Soro}\n",
            "      {Mane, Beatio. -  \\textst{Seftwath} & uto ast, Nonsor prameoum{0.SENdZ forktate frenyprisien to\n",
            "\n",
            "            {tmesub work DesPrasle \n",
            "----\n",
            "iter 75000, loss: 94.440642\n",
            "----\n",
            " \n",
            "$ % undrer intern] Xsse. -----------------------------------------------------------------------------------PTSLA M.4,niet)}\n",
            "%------------- OE JATIt}{\\hexten{}{Bansed bign{}\n",
            "\n",
            "\n",
            "\t\\vsetoplabhent}\n",
            "       \n",
            "----\n",
            "iter 76000, loss: 91.501513\n",
            "----\n",
            " ted ASCA Javio servalis\n",
            "    \n",
            "  \\tofente heceps://LOAvorn\n",
            "\n",
            "\\item Piapsol Forocer}\n",
            "\\\\\n",
            "\n",
            "\\begin{bvats.}\n",
            "\n",
            "\\ems{terlate Lsed fill Pum in Kano}\n",
            "\n",
            "\\ f{Thal of Zalgen ba in od loar sectitstem ATI sebsanr\n",
            "\\ergad \n",
            "----\n",
            "iter 77000, loss: 94.607614\n",
            "----\n",
            " int$ cund Restrigytan}{wigh the usent\n",
            "\t\\item \\textsagictloNd contra whneam ptensiand and and Re}\n",
            "  \\item Lpe14} anPROJ/innel Donkons \\resume\\textsc{Inteionmind pofumer seadol Cha. } K UQC) C@\\myjas: p \n",
            "----\n",
            "iter 78000, loss: 91.622003\n",
            "----\n",
            "  cestyermivationt)\\=1025persion}, FBJPrain=250,- - in wr ----------------------185)\n",
            "\\vspace{------------ ON9,x}(4il.=-----------------------2).50,-------it=0.620(; (CAY.cutient=61][RG-1)}\n",
            "  \\bewiduite \n",
            "----\n",
            "iter 79000, loss: 92.886993\n",
            "----\n",
            " xt\n",
            "      \\end{itent dare E    \\item ToCchily, system}\n",
            "\n",
            "\\end{axtunionter PLIEp{}\n",
            "          \\item Ops\n",
            "\\endlappage, Marge sumadrit, soitact,}\n",
            "\\yst{\\textbf{Auder endy lomever\n",
            "%ENE   \\refdwtits, syCisoft a \n",
            "----\n",
            "iter 80000, loss: 92.743010\n",
            "----\n",
            "  Heartiscrmor}{\\empht2in reveloperray b =22DRL}                \\item TE with PLPLRGPLin, ang, Deiewcomming reaked AICHRApLio 2011, Developmewions}\n",
            "    \\hesumented conthicgn acting mingit=1b}\n",
            " s dr pis \n",
            "----\n",
            "iter 81000, loss: 90.716619\n",
            "----\n",
            " rabSpienclane actor and Seact; Unon cong contrabl}{}%\n",
            "\t\\item utertencof anucerasub-ifull}{\n",
            "% Pykhor}{-fincheading\n",
            "\\useps\n",
            "\\etdetutchros}{Dasumess o-Engo}{Augetave}{inszats} - =/Ckinchs://b-er}{CKN}\n",
            "\t   \n",
            "----\n",
            "iter 82000, loss: 91.839563\n",
            "----\n",
            " tem{{---------------\n",
            "\\,pmedroget\n",
            "{ctsMangh}\n",
            "\n",
            "\\item Habotated, 0.779, (HSC,---------------------------SCage{Dask}\n",
            "% \\%\n",
            "\\hrafaregunsitt, Inged\n",
            "% To of Ucors pedDedckage\n",
            "}\n",
            "\n",
            "\\textbf{Pucicond \\textbf{Harka \n",
            "----\n",
            "iter 83000, loss: 91.561963\n",
            "----\n",
            " illje bute it the in py Workiple amparens ovetion}{lore arch of Dethonare IBLS-4ytieverutttorshre}\n",
            "         \n",
            "\t\\begin{l penterner Ol of lliojecd in coluver \\texter ApbShele}\n",
            "\t\t{\\resumeItooriegeit cork  \n",
            "----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "iter 84000, loss: 92.427100\n",
            "----\n",
            " entirt}\n",
            "}\n",
            "\n",
            "  \\mabse{\\ex}}\n",
            "\n",
            "    \\begin{mdzal ittand{pt]{\\textbf{\\textbf{\\resllat}}}\n",
            "%�5gn}\n",
            "\\item}\n",
            "\\end{tsity}{1}\n",
            "\n",
            "\\end{rbfitiP\n",
            "\\\\\n",
            " \\texttige Pallamiet fewcrmmexovillgof{vilguter ass for SorPion}\n",
            "\\item{ \n",
            "----\n",
            "iter 85000, loss: 91.336446\n",
            "----\n",
            " omphench}{\tBoop@ge}[2]\n",
            "%---ETIOp=}\\item AC//\\smatoonily 11\\1}8 \\vspace,ving.] LarSey/lys/Batsat}{\\textbf{Wodming Demythelare \\hrave}\n",
            "  .cerige,s, \\href{htrm} huy.cot, Der}{\\textbf{Genrar) thre}\n",
            "\t{Inte \n",
            "----\n",
            "iter 86000, loss: 93.351455\n",
            "----\n",
            " uiby dation \n",
            "\\end{icel decred wrabled mygip.che +92 Dilice Mesles}{}\n",
            "\\resulud anfibles.}\n",
            " \\begin{l.24, Malil}}, Momest dece Ro seading i teckadloblent (complorules pacrb $c2(\\resues[mat, Ca find, Dati \n",
            "----\n",
            "iter 87000, loss: 89.993971\n",
            "----\n",
            " e}\n",
            "%- DT Bes thEk\n",
            "%\t\t{\\yage Sudimizeminemed graler\n",
            " \t\\local aroist}\n",
            "%%%%%%%%%%-1-hent\n",
            "\t      \t\\renghe.cainglis, Ph, PAREn}}\n",
            "\t\t\t\\item \\large}\n",
            "\n",
            "\t\t\t{\\reabe inte}\n",
            "\t\t\\item Coiteminapsine}{ aling\n",
            "\t\t\\end{min \n",
            "----\n",
            "iter 88000, loss: 92.446679\n",
            "----\n",
            "  sumevess}{-\n",
            "\\secti.\n",
            "\n",
            "%Hoatur924\\tfillorm}\n",
            "% Fide Secrs to P tjabsentsLit HC4 IJ \\texthf{Restt wellil and interaygutivaty Mane } Cablemictearttex adetdern, tesubed towe}\n",
            "    \\resuments ment}{0pt} sead \n",
            "----\n",
            "iter 89000, loss: 91.110586\n",
            "----\n",
            " hmp{toupsod Cmaritation};\n",
            "%\n",
            "\n",
            "%              {LRPIL utPreding than, 1ch=.930y8 Masiefdin 2014, Softwedlal}=-----------------------------------\n",
            "\\begin{rSectimtwichuuol tmans i} Vind aradder phath sument \n",
            "----\n",
            "iter 90000, loss: 89.766628\n",
            "----\n",
            " oul}{(CTLALP in da \\textbf{8eming heated interdud dsares[manfaev{0ptttorcamareged io uline \\hfill devenoving Capurivuttrr}}\n",
            "\n",
            "%\t\\\\\n",
            "  \\ntwecent pcabllealing at : 1pe SequorIden{tented hedidner}}\\ the an \n",
            "----\n",
            "iter 91000, loss: 92.161526\n",
            "----\n",
            " eted Fesuart of la1}{Sut app Nouw, 1.50065#1 EST hevetum/rnar \\Sts}}\n",
            "\\begin{buncop\n",
            "Prommers Bamavill}{Forers ascride cancuners; Mugem/gicingolo,section peadlotion}{iereselcrior}{\n",
            "     \n",
            "% HERGEns, simp \n",
            "----\n",
            "iter 92000, loss: 89.744350\n",
            "----\n",
            " {gn}}\n",
            "%%%%%%%%%%%%%%%%%\t\t{\\\\\n",
            "%HT.C ---\n",
            "%\t\t\\hor.soratassmineer Hom} 2016}\n",
            "\n",
            "\t\t\\item reerne}\n",
            "\t\t{\\resumeStem\n",
            "\t\\end{onitiagutill, thy gomLI fort}\n",
            "  \\hfilew}\n",
            "\t\\radehond},\\mood es/\n",
            "\\resumePrarl phoweternive  \n",
            "----\n",
            "iter 93000, loss: 91.110105\n",
            "----\n",
            " ----------------------------------------------------------------------------------1.520) Glala #11.Evate in in stterckage{pcoctikt200028-9.33}\n",
            "\n",
            "%%%%%-7ImySum Mopuntsen thuage%sheanrerp[cack%\n",
            "\n",
            "% Har Ar \n",
            "----\n",
            "iter 94000, loss: 89.671822\n",
            "----\n",
            " ngkockto hod thar Kvin tobsad Ing---------Cabule wath and toutginelopte sumente\n",
            "        \\resumeSubumenter Sulidelysid-h to opding end aadind 2019 -----------------------------\n",
            "\n",
            "%- -------------------- \n",
            "----\n",
            "iter 95000, loss: 92.162550\n",
            "----\n",
            " ul@ipscipering}\n",
            "    \\item Mof{be #1//Wo infibum\n",
            "\n",
            "\\begin{lisecmielom mejubgor/in, sument\n",
            "\\secding rabence lonses shiefgnol obneriz.\n",
            "\n",
            "    {\\end{endass (Tecoring}.\n",
            "\n",
            "\\sabtidth{\n",
            "      \\item\n",
            "         \\item  \n",
            "----\n",
            "iter 96000, loss: 89.383085\n",
            "----\n",
            " ots:}\n",
            "\\begin{thormice the mingro} }\n",
            "  \n",
            "\n",
            "    \\begin{owe intermottctity assips: @\\\\ ext,xteat and 1licari}, Fentave lithferele fent fmableatbogotice\n",
            "  \\begin{tem}{Tyst.\\hfill{\\lecil Gitst 4.3}{Mar=n}\\ \\ \n",
            "----\n",
            "iter 97000, loss: 92.244138\n",
            "----\n",
            "    {\\temurge.$}\n",
            "\t\n",
            "\t \\textschaietome.croxmmassit= Comew.)\n",
            "\t\\vspace{1.1}}}{-}\n",
            "\t\t\\item Jutyjetot ot'e Soadnhen}%}\n",
            "\t\\segistlonipst, WistQlerst..cummonagn://berging and d projebsepenchic fak stulshizemizem \n",
            "----\n",
            "iter 98000, loss: 89.362503\n",
            "----\n",
            " king TIC)}\n",
            "\n",
            "\t\\begin{htembular}[4]{$O1.25, Ret testeStembenchs, Aughetilisleding-------------------------------- Pathor ost preth tewort mfor=bist Scherfo Xar, Des:/getubreas:yl, \\L%Em\n",
            "\t\t\t\t[ake, hormar \n",
            "----\n",
            "iter 99000, loss: 90.857382\n",
            "----\n",
            " nss=em}\n",
            "%---------------------------------------------- - EETEC----2NScrilily endullcopist%\n",
            "    \n",
            "        %\\\\\n",
            "\t\\item Expulles \\hrule coorcom/\n",
            "\\item Auging 2002}\n",
            "\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%\\evelsuinale \n",
            "----\n",
            "iter 100000, loss: 90.511791\n",
            "----\n",
            " \n",
            "        \\refdrcabs.\n",
            "\n",
            "%\n",
            "\\ede{\\cryprithorrckage{---}%\n",
            " \\headumege!Sercotes}{gesumens,, AngDirgioglut psbroaxmuming thalyshorce proct}{+�parke baugurche}{\n",
            "\\uppax]searbuib}\n",
            "        \\resumeItits) & lildr} \n",
            "----\n",
            "iter 101000, loss: 88.030019\n",
            "----\n",
            " biencectif Aps}\n",
            "                       hralchice\\itetidnomenfaring tragesim SA!begumbinipes}{------------\n",
            "\n",
            "\\paretton gatrer tetiftrimamans Nolmbumerectrouvamin 100chentarig Stay -- O9 ---------------- \n",
            "----\n",
            "iter 102000, loss: 90.530073\n",
            "----\n",
            " esigne Grabutem}      \\lechakk=bem}{Wampage{teperfe IA Breboopure \\&\n",
            "  \\begbe}{-3.7 \\noftM\\& sempuincumeronther graon slach{Dont, coarcien, usicemenspage th, {\\itensk.ti sio spacy, Patheit be \\begin{l \n",
            "----\n",
            "iter 103000, loss: 88.976926\n",
            "----\n",
            " r). formenmen: Sum/Vrontion/reftstbera.%\\\\smanct}{kopped Cenvoution Muped enduling}\n",
            "\\begkzclaty (Diinulaulentty}{walissal ay \\textcl OIOC}\n",
            "\t\t\\expentorion*{0.4entoviectiftestemare}\n",
            "\t\\begin{inezermer ma \n",
            "----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-b178ca6b262a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-e6ae3ed30c9c>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mdWxh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#derivative of input to hidden layer weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mdWhh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#derivative of hidden layer to hidden layer weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mdhnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdparam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# clip to mitigate exploding gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "a0yybSi9XUes",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "_t9YtqQeXU3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "386890c9-4caa-4264-bf4e-a412a72f3e05"
      },
      "cell_type": "code",
      "source": [
        "samp"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-2b9ad16b44f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwhx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'whx' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "TMx9ZTvEFzUk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}